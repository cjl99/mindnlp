{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import mindspore\n",
    "from mindspore.nn import AdamWeightDecay\n",
    "from mindnlp import load_dataset, process\n",
    "from tqdm import tqdm\n",
    "# from mindnlp.metrics import \n",
    "\n",
    "from mindnlp.peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    PeftType,\n",
    ")\n",
    "from mindnlp.dataset import MRPC, MRPC_Process\n",
    "\n",
    "from mindnlp.transforms import RobertaTokenizer\n",
    "from mindnlp.models import RobertaConfig, RobertaForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model_name_or_path = \"roberta-large\"\n",
    "task = \"mrpc\"\n",
    "peft_type = PeftType.LORA\n",
    "device = \"GPU\" # \"cuda\"\n",
    "num_epochs = 20\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "mrpc_train, mrpc_test = MRPC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds col names: ['label', 'sentence1', 'sentence2']\n",
      "length of train ds: 4076 length of test ds: 1725\n",
      "1 Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence . Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n",
      "0 Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion . Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\n",
      "1 They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added . On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\n",
      "0 Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 . Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .\n",
      "1 The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange . PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .\n",
      "1 Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier . With the scandal hanging over Stewart 's company , revenue the first quarter of the year dropped 15 percent from the same period a year earlier .\n"
     ]
    }
   ],
   "source": [
    "# take a brief look at the dataset\n",
    "print(\"ds col names:\", mrpc_train.column_names)\n",
    "print(\"length of train ds:\", len(mrpc_train), \"length of test ds:\", len(mrpc_test))\n",
    "\n",
    "iter = mrpc_train.create_tuple_iterator()\n",
    "for i, (l, s1, s2) in enumerate(iter):\n",
    "    if i <= 5:\n",
    "        print(l, s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "cols = ['sentence1', 'sentence2']\n",
    "def process_dataset(dataset, tokenizer, column_names, batch_size, max_seq_len=512, shuffle=False):\n",
    "    # tokenize\n",
    "    for col in column_names:\n",
    "        dataset = dataset.map(tokenizer, input_columns=col)\n",
    "\n",
    "    # \n",
    "    return dataset\n",
    "\n",
    "ds = process_dataset(mrpc_train, tokenizer, column_names=cols, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## roberta-base model from pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(2808750:140565782538048,MainProcess):2023-09-09-20:07:55.869.335 [/home/cjl/code/mind/mindnlp/mindnlp/abc/models/pretrained_model.py:454] The following parameters in checkpoint files are not loaded:\n",
      "['roberta.encoder.layer.12.attention.self_attn.query.weight', 'roberta.encoder.layer.12.attention.self_attn.query.bias', 'roberta.encoder.layer.12.attention.self_attn.key.weight', 'roberta.encoder.layer.12.attention.self_attn.key.bias', 'roberta.encoder.layer.12.attention.self_attn.value.weight', 'roberta.encoder.layer.12.attention.self_attn.value.bias', 'roberta.encoder.layer.12.attention.output.dense.weight', 'roberta.encoder.layer.12.attention.output.dense.bias', 'roberta.encoder.layer.12.attention.output.layer_norm.gamma', 'roberta.encoder.layer.12.attention.output.layer_norm.beta', 'roberta.encoder.layer.12.intermediate.dense.weight', 'roberta.encoder.layer.12.intermediate.dense.bias', 'roberta.encoder.layer.12.output.dense.weight', 'roberta.encoder.layer.12.output.dense.bias', 'roberta.encoder.layer.12.output.layer_norm.gamma', 'roberta.encoder.layer.12.output.layer_norm.beta', 'roberta.encoder.layer.13.attention.self_attn.query.weight', 'roberta.encoder.layer.13.attention.self_attn.query.bias', 'roberta.encoder.layer.13.attention.self_attn.key.weight', 'roberta.encoder.layer.13.attention.self_attn.key.bias', 'roberta.encoder.layer.13.attention.self_attn.value.weight', 'roberta.encoder.layer.13.attention.self_attn.value.bias', 'roberta.encoder.layer.13.attention.output.dense.weight', 'roberta.encoder.layer.13.attention.output.dense.bias', 'roberta.encoder.layer.13.attention.output.layer_norm.gamma', 'roberta.encoder.layer.13.attention.output.layer_norm.beta', 'roberta.encoder.layer.13.intermediate.dense.weight', 'roberta.encoder.layer.13.intermediate.dense.bias', 'roberta.encoder.layer.13.output.dense.weight', 'roberta.encoder.layer.13.output.dense.bias', 'roberta.encoder.layer.13.output.layer_norm.gamma', 'roberta.encoder.layer.13.output.layer_norm.beta', 'roberta.encoder.layer.14.attention.self_attn.query.weight', 'roberta.encoder.layer.14.attention.self_attn.query.bias', 'roberta.encoder.layer.14.attention.self_attn.key.weight', 'roberta.encoder.layer.14.attention.self_attn.key.bias', 'roberta.encoder.layer.14.attention.self_attn.value.weight', 'roberta.encoder.layer.14.attention.self_attn.value.bias', 'roberta.encoder.layer.14.attention.output.dense.weight', 'roberta.encoder.layer.14.attention.output.dense.bias', 'roberta.encoder.layer.14.attention.output.layer_norm.gamma', 'roberta.encoder.layer.14.attention.output.layer_norm.beta', 'roberta.encoder.layer.14.intermediate.dense.weight', 'roberta.encoder.layer.14.intermediate.dense.bias', 'roberta.encoder.layer.14.output.dense.weight', 'roberta.encoder.layer.14.output.dense.bias', 'roberta.encoder.layer.14.output.layer_norm.gamma', 'roberta.encoder.layer.14.output.layer_norm.beta', 'roberta.encoder.layer.15.attention.self_attn.query.weight', 'roberta.encoder.layer.15.attention.self_attn.query.bias', 'roberta.encoder.layer.15.attention.self_attn.key.weight', 'roberta.encoder.layer.15.attention.self_attn.key.bias', 'roberta.encoder.layer.15.attention.self_attn.value.weight', 'roberta.encoder.layer.15.attention.self_attn.value.bias', 'roberta.encoder.layer.15.attention.output.dense.weight', 'roberta.encoder.layer.15.attention.output.dense.bias', 'roberta.encoder.layer.15.attention.output.layer_norm.gamma', 'roberta.encoder.layer.15.attention.output.layer_norm.beta', 'roberta.encoder.layer.15.intermediate.dense.weight', 'roberta.encoder.layer.15.intermediate.dense.bias', 'roberta.encoder.layer.15.output.dense.weight', 'roberta.encoder.layer.15.output.dense.bias', 'roberta.encoder.layer.15.output.layer_norm.gamma', 'roberta.encoder.layer.15.output.layer_norm.beta', 'roberta.encoder.layer.16.attention.self_attn.query.weight', 'roberta.encoder.layer.16.attention.self_attn.query.bias', 'roberta.encoder.layer.16.attention.self_attn.key.weight', 'roberta.encoder.layer.16.attention.self_attn.key.bias', 'roberta.encoder.layer.16.attention.self_attn.value.weight', 'roberta.encoder.layer.16.attention.self_attn.value.bias', 'roberta.encoder.layer.16.attention.output.dense.weight', 'roberta.encoder.layer.16.attention.output.dense.bias', 'roberta.encoder.layer.16.attention.output.layer_norm.gamma', 'roberta.encoder.layer.16.attention.output.layer_norm.beta', 'roberta.encoder.layer.16.intermediate.dense.weight', 'roberta.encoder.layer.16.intermediate.dense.bias', 'roberta.encoder.layer.16.output.dense.weight', 'roberta.encoder.layer.16.output.dense.bias', 'roberta.encoder.layer.16.output.layer_norm.gamma', 'roberta.encoder.layer.16.output.layer_norm.beta', 'roberta.encoder.layer.17.attention.self_attn.query.weight', 'roberta.encoder.layer.17.attention.self_attn.query.bias', 'roberta.encoder.layer.17.attention.self_attn.key.weight', 'roberta.encoder.layer.17.attention.self_attn.key.bias', 'roberta.encoder.layer.17.attention.self_attn.value.weight', 'roberta.encoder.layer.17.attention.self_attn.value.bias', 'roberta.encoder.layer.17.attention.output.dense.weight', 'roberta.encoder.layer.17.attention.output.dense.bias', 'roberta.encoder.layer.17.attention.output.layer_norm.gamma', 'roberta.encoder.layer.17.attention.output.layer_norm.beta', 'roberta.encoder.layer.17.intermediate.dense.weight', 'roberta.encoder.layer.17.intermediate.dense.bias', 'roberta.encoder.layer.17.output.dense.weight', 'roberta.encoder.layer.17.output.dense.bias', 'roberta.encoder.layer.17.output.layer_norm.gamma', 'roberta.encoder.layer.17.output.layer_norm.beta', 'roberta.encoder.layer.18.attention.self_attn.query.weight', 'roberta.encoder.layer.18.attention.self_attn.query.bias', 'roberta.encoder.layer.18.attention.self_attn.key.weight', 'roberta.encoder.layer.18.attention.self_attn.key.bias', 'roberta.encoder.layer.18.attention.self_attn.value.weight', 'roberta.encoder.layer.18.attention.self_attn.value.bias', 'roberta.encoder.layer.18.attention.output.dense.weight', 'roberta.encoder.layer.18.attention.output.dense.bias', 'roberta.encoder.layer.18.attention.output.layer_norm.gamma', 'roberta.encoder.layer.18.attention.output.layer_norm.beta', 'roberta.encoder.layer.18.intermediate.dense.weight', 'roberta.encoder.layer.18.intermediate.dense.bias', 'roberta.encoder.layer.18.output.dense.weight', 'roberta.encoder.layer.18.output.dense.bias', 'roberta.encoder.layer.18.output.layer_norm.gamma', 'roberta.encoder.layer.18.output.layer_norm.beta', 'roberta.encoder.layer.19.attention.self_attn.query.weight', 'roberta.encoder.layer.19.attention.self_attn.query.bias', 'roberta.encoder.layer.19.attention.self_attn.key.weight', 'roberta.encoder.layer.19.attention.self_attn.key.bias', 'roberta.encoder.layer.19.attention.self_attn.value.weight', 'roberta.encoder.layer.19.attention.self_attn.value.bias', 'roberta.encoder.layer.19.attention.output.dense.weight', 'roberta.encoder.layer.19.attention.output.dense.bias', 'roberta.encoder.layer.19.attention.output.layer_norm.gamma', 'roberta.encoder.layer.19.attention.output.layer_norm.beta', 'roberta.encoder.layer.19.intermediate.dense.weight', 'roberta.encoder.layer.19.intermediate.dense.bias', 'roberta.encoder.layer.19.output.dense.weight', 'roberta.encoder.layer.19.output.dense.bias', 'roberta.encoder.layer.19.output.layer_norm.gamma', 'roberta.encoder.layer.19.output.layer_norm.beta', 'roberta.encoder.layer.20.attention.self_attn.query.weight', 'roberta.encoder.layer.20.attention.self_attn.query.bias', 'roberta.encoder.layer.20.attention.self_attn.key.weight', 'roberta.encoder.layer.20.attention.self_attn.key.bias', 'roberta.encoder.layer.20.attention.self_attn.value.weight', 'roberta.encoder.layer.20.attention.self_attn.value.bias', 'roberta.encoder.layer.20.attention.output.dense.weight', 'roberta.encoder.layer.20.attention.output.dense.bias', 'roberta.encoder.layer.20.attention.output.layer_norm.gamma', 'roberta.encoder.layer.20.attention.output.layer_norm.beta', 'roberta.encoder.layer.20.intermediate.dense.weight', 'roberta.encoder.layer.20.intermediate.dense.bias', 'roberta.encoder.layer.20.output.dense.weight', 'roberta.encoder.layer.20.output.dense.bias', 'roberta.encoder.layer.20.output.layer_norm.gamma', 'roberta.encoder.layer.20.output.layer_norm.beta', 'roberta.encoder.layer.21.attention.self_attn.query.weight', 'roberta.encoder.layer.21.attention.self_attn.query.bias', 'roberta.encoder.layer.21.attention.self_attn.key.weight', 'roberta.encoder.layer.21.attention.self_attn.key.bias', 'roberta.encoder.layer.21.attention.self_attn.value.weight', 'roberta.encoder.layer.21.attention.self_attn.value.bias', 'roberta.encoder.layer.21.attention.output.dense.weight', 'roberta.encoder.layer.21.attention.output.dense.bias', 'roberta.encoder.layer.21.attention.output.layer_norm.gamma', 'roberta.encoder.layer.21.attention.output.layer_norm.beta', 'roberta.encoder.layer.21.intermediate.dense.weight', 'roberta.encoder.layer.21.intermediate.dense.bias', 'roberta.encoder.layer.21.output.dense.weight', 'roberta.encoder.layer.21.output.dense.bias', 'roberta.encoder.layer.21.output.layer_norm.gamma', 'roberta.encoder.layer.21.output.layer_norm.beta', 'roberta.encoder.layer.22.attention.self_attn.query.weight', 'roberta.encoder.layer.22.attention.self_attn.query.bias', 'roberta.encoder.layer.22.attention.self_attn.key.weight', 'roberta.encoder.layer.22.attention.self_attn.key.bias', 'roberta.encoder.layer.22.attention.self_attn.value.weight', 'roberta.encoder.layer.22.attention.self_attn.value.bias', 'roberta.encoder.layer.22.attention.output.dense.weight', 'roberta.encoder.layer.22.attention.output.dense.bias', 'roberta.encoder.layer.22.attention.output.layer_norm.gamma', 'roberta.encoder.layer.22.attention.output.layer_norm.beta', 'roberta.encoder.layer.22.intermediate.dense.weight', 'roberta.encoder.layer.22.intermediate.dense.bias', 'roberta.encoder.layer.22.output.dense.weight', 'roberta.encoder.layer.22.output.dense.bias', 'roberta.encoder.layer.22.output.layer_norm.gamma', 'roberta.encoder.layer.22.output.layer_norm.beta', 'roberta.encoder.layer.23.attention.self_attn.query.weight', 'roberta.encoder.layer.23.attention.self_attn.query.bias', 'roberta.encoder.layer.23.attention.self_attn.key.weight', 'roberta.encoder.layer.23.attention.self_attn.key.bias', 'roberta.encoder.layer.23.attention.self_attn.value.weight', 'roberta.encoder.layer.23.attention.self_attn.value.bias', 'roberta.encoder.layer.23.attention.output.dense.weight', 'roberta.encoder.layer.23.attention.output.dense.bias', 'roberta.encoder.layer.23.attention.output.layer_norm.gamma', 'roberta.encoder.layer.23.attention.output.layer_norm.beta', 'roberta.encoder.layer.23.intermediate.dense.weight', 'roberta.encoder.layer.23.intermediate.dense.bias', 'roberta.encoder.layer.23.output.dense.weight', 'roberta.encoder.layer.23.output.dense.bias', 'roberta.encoder.layer.23.output.layer_norm.gamma', 'roberta.encoder.layer.23.output.layer_norm.beta', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.gamma', 'lm_head.layer_norm.beta', 'lm_head.decoder.weight']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_config = RobertaConfig(num_labels=2)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', config=model_config )\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m peft_config \u001b[39m=\u001b[39m LoraConfig(task_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSEQ_CLS\u001b[39m\u001b[39m\"\u001b[39m, inference_mode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, r\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, lora_alpha\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, lora_dropout\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m peft_model \u001b[39m=\u001b[39m get_peft_model(model, peft_config)\n\u001b[1;32m      3\u001b[0m \u001b[39m# model.print_train_parameters()\u001b[39;00m\n",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/mapping.py:95\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[39mreturn\u001b[39;00m PeftModel(model, peft_config, adapter_name\u001b[39m=\u001b[39madapter_name)\n\u001b[1;32m     92\u001b[0m \u001b[39m# TODO: prompt learning\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m# if peft_config.is_prompt_learning:\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m#     # peft_config = _prepare_prompt_learning_config(peft_config, model_config)\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[39mreturn\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config\u001b[39m.\u001b[39;49mtask_type](model, peft_config, adapter_name\u001b[39m=\u001b[39;49madapter_name)\n",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/peft_model.py:280\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model, peft_config: PeftConfig, adapter_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 280\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, peft_config, adapter_name)\n\u001b[1;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodules_to_save \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodules_to_save \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mclassifier\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m}\n",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/peft_model.py:80\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m peft_config\u001b[39m.\u001b[39mis_prompt_learning:\n\u001b[1;32m     79\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config[adapter_name] \u001b[39m=\u001b[39m peft_config\n\u001b[0;32m---> 80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model \u001b[39m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[39m.\u001b[39;49mpeft_type](\n\u001b[1;32m     81\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpeft_config, adapter_name\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m     84\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/tuners/lora.py:168\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model: nn\u001b[39m.\u001b[39mCell, config, adapter_name):\n\u001b[1;32m    166\u001b[0m     \u001b[39m# call BaseTuner.__init__\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# setup config and inject lora adapter\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, config, adapter_name)\n",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/tuners/tuners_utils.py:93\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config\u001b[39m.\u001b[39mupdate(peft_config)\n\u001b[1;32m     89\u001b[0m \u001b[39m# transformers models have a .config attribute, whose presence is assumed later on\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m# if not hasattr(self, \"config\"):\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m#     self.config = {\"model_type\": \"custom\"}\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minject_adapter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, adapter_name)\n\u001b[1;32m     95\u001b[0m \u001b[39m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpeft_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/tuners/tuners_utils.py:229\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model_config, \u001b[39m\"\u001b[39m\u001b[39mto_dict\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    227\u001b[0m     model_config \u001b[39m=\u001b[39m model_config\u001b[39m.\u001b[39mto_dict()\n\u001b[0;32m--> 229\u001b[0m peft_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_adapter_config(peft_config, model_config) \u001b[39m# pylint: disable=E1111\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_list:\n\u001b[1;32m    232\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_target_module_exists(peft_config, key):\n",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/tuners/lora.py:174\u001b[0m, in \u001b[0;36mLoraModel._prepare_adapter_config\u001b[0;34m(peft_config, model_config)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_adapter_config\u001b[39m(peft_config, model_config):\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;00m peft_config\u001b[39m.\u001b[39mtarget_modules \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m         \u001b[39m# If target_modules is not specified, use the default target_modules for the model type\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m         \u001b[39mif\u001b[39;00m model_config[\u001b[39m\"\u001b[39;49m\u001b[39mmodel_type\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING:\n\u001b[1;32m    175\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPlease specify `target_modules` in `peft_config`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m         peft_config\u001b[39m.\u001b[39mtarget_modules \u001b[39m=\u001b[39m TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[model_config[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m]]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_type'"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1)\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "# model.print_train_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
