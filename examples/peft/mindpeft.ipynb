{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "from mindspore import context, Tensor\n",
    "from mindspore.communication import init, get_rank\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")\n",
    "# init('nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32\n",
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<mindspore.dataset.engine.datasets_user_defined.GeneratorDataset at 0x7f3fe4891df0>,\n",
       " <mindspore.dataset.engine.datasets.BatchDataset at 0x7f3fe4891eb0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mindspore 构造一个测试数据集\n",
    "\n",
    "X = mindspore.ops.rand((1000,20), seed=0)\n",
    "y = (X.sum(1) > 10).int()\n",
    "\n",
    "n_train = 800\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "max_epochs = 100\n",
    "\n",
    "class MyIterable:\n",
    "    def __init__(self, X: Tensor, y:Tensor):\n",
    "        self._index = 0\n",
    "        self._data = X\n",
    "        self._label = y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._index >= len(self._data):\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            item = (self._data[self._index], self._label[self._index])\n",
    "            self._index += 1\n",
    "            return item\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._index = 0\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "\n",
    "train_ds = ds.GeneratorDataset(\n",
    "    source=MyIterable(X[:n_train], y[:n_train]),\n",
    "    column_names=[\"data\", \"label\"],\n",
    "    shuffle=True\n",
    ")\n",
    "eval_ds = ds.GeneratorDataset(\n",
    "    source=MyIterable(X[n_train:], y[n_train:]),\n",
    "    column_names=[\"data\", \"label\"],\n",
    ")\n",
    "\n",
    "train_dataloader = train_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "eval_dataloader = eval_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "train_ds, train_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_net_params(net: nn.Cell):\n",
    "    all_parameter = []\n",
    "    for item in net.get_parameters():\n",
    "        all_parameter.append(item)\n",
    "        print(item.name, item.data.shape)\n",
    "    print(f\"all parameter numbers: {len(all_parameter)}\")\n",
    "\n",
    "    # Obtain trainable parameters.\n",
    "    trainable_params = net.trainable_params()\n",
    "    for item in trainable_params:\n",
    "        print(item.name, item.data.shape)\n",
    "    print(f\"trainable parameter numbers: {len(trainable_params)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Cell):\n",
    "    def __init__(self, hidden=2000):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.SequentialCell(\n",
    "            nn.Dense(20, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(hidden, 2),\n",
    "            nn.LogSoftmax(axis=-1)\n",
    "        )\n",
    "\n",
    "    def construct(self, X):\n",
    "        return self.layers(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight (2000, 20)\n",
      "layers.0.bias (2000,)\n",
      "layers.2.weight (2000, 2000)\n",
      "layers.2.bias (2000,)\n",
      "layers.4.weight (2, 2000)\n",
      "layers.4.bias (2,)\n",
      "all parameter numbers: 6\n",
      "layers.0.weight (2000, 20)\n",
      "layers.0.bias (2000,)\n",
      "layers.2.weight (2000, 2000)\n",
      "layers.2.bias (2000,)\n",
      "layers.4.weight (2, 2000)\n",
      "layers.4.bias (2,)\n",
      "trainable parameter numbers: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mlp = MLP()\n",
    "print_net_params(mlp)\n",
    "optimizer = nn.Adam(mlp.trainable_params(), learning_rate=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, optimizer, criterion, train_dataloader, eval_dataloader, epochs):\n",
    "    def forward_fn(data, label):\n",
    "        logits = model(data)\n",
    "        loss = criterion(logits, label)\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "    def train_one_epoch():\n",
    "        model.set_train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in train_dataloader:\n",
    "            # forward + compute grad\n",
    "            (loss, logits), grad = grad_fn(xb, yb)\n",
    "            # update model params\n",
    "            optimizer(grad)\n",
    "            total_loss += loss\n",
    "        \n",
    "        return total_loss / len(train_dataloader)\n",
    "    \n",
    "    def eval_one_epoch():\n",
    "        model.set_train(False)\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in eval_dataloader:\n",
    "            (loss, logits), grad = grad_fn(xb, yb)\n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss / len(eval_dataloader)\n",
    "\n",
    "    # train start from here\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch()\n",
    "        eval_loss = eval_one_epoch()\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"epoch:{epoch}  train_loss:{train_loss}  eval_loss:{eval_loss}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0  train_loss:0.69208336  eval_loss:0.6873576\n",
      "epoch:2  train_loss:0.6827678  eval_loss:0.67769283\n",
      "epoch:4  train_loss:0.6582739  eval_loss:0.6506631\n",
      "epoch:6  train_loss:0.6083377  eval_loss:0.5987927\n",
      "epoch:8  train_loss:0.53343755  eval_loss:0.52710015\n",
      "epoch:10  train_loss:0.44243312  eval_loss:0.43978286\n",
      "epoch:12  train_loss:0.3544993  eval_loss:0.36559346\n",
      "epoch:14  train_loss:0.2854956  eval_loss:0.31712672\n",
      "epoch:16  train_loss:0.23512037  eval_loss:0.28558853\n",
      "epoch:18  train_loss:0.1979201  eval_loss:0.2649181\n",
      "epoch:20  train_loss:0.16955899  eval_loss:0.25108624\n",
      "epoch:22  train_loss:0.14691924  eval_loss:0.24156027\n",
      "epoch:24  train_loss:0.12857452  eval_loss:0.23532297\n",
      "epoch:26  train_loss:0.113222696  eval_loss:0.2311116\n",
      "epoch:28  train_loss:0.10017309  eval_loss:0.22861288\n",
      "epoch:30  train_loss:0.08903113  eval_loss:0.2275504\n",
      "epoch:32  train_loss:0.07936326  eval_loss:0.22722642\n",
      "epoch:34  train_loss:0.07092509  eval_loss:0.22780854\n",
      "epoch:36  train_loss:0.06348407  eval_loss:0.22907849\n",
      "epoch:38  train_loss:0.056972608  eval_loss:0.23089051\n",
      "epoch:40  train_loss:0.051228974  eval_loss:0.23282437\n",
      "epoch:42  train_loss:0.046135545  eval_loss:0.23529442\n",
      "epoch:44  train_loss:0.04163889  eval_loss:0.23802924\n",
      "epoch:46  train_loss:0.037610296  eval_loss:0.2413766\n",
      "epoch:48  train_loss:0.034131985  eval_loss:0.24527343\n",
      "epoch:50  train_loss:0.030993514  eval_loss:0.24872398\n",
      "epoch:52  train_loss:0.02818674  eval_loss:0.25237265\n",
      "epoch:54  train_loss:0.02574637  eval_loss:0.2569733\n",
      "epoch:56  train_loss:0.023615455  eval_loss:0.26163217\n",
      "epoch:58  train_loss:0.021683054  eval_loss:0.26608622\n",
      "epoch:60  train_loss:0.019974057  eval_loss:0.27119705\n",
      "epoch:62  train_loss:0.018464087  eval_loss:0.27651152\n",
      "epoch:64  train_loss:0.017138716  eval_loss:0.28218126\n",
      "epoch:66  train_loss:0.015992431  eval_loss:0.28807488\n",
      "epoch:68  train_loss:0.014973049  eval_loss:0.2945722\n",
      "epoch:70  train_loss:0.014108867  eval_loss:0.30169925\n",
      "epoch:72  train_loss:0.013356765  eval_loss:0.3086976\n",
      "epoch:74  train_loss:0.012720797  eval_loss:0.31646493\n",
      "epoch:76  train_loss:0.01221386  eval_loss:0.32433605\n",
      "epoch:78  train_loss:0.011782183  eval_loss:0.3323916\n",
      "epoch:80  train_loss:0.011471827  eval_loss:0.34123358\n",
      "epoch:82  train_loss:0.01130312  eval_loss:0.35028443\n",
      "epoch:84  train_loss:0.011199843  eval_loss:0.35907713\n",
      "epoch:86  train_loss:0.01123751  eval_loss:0.36772934\n",
      "epoch:88  train_loss:0.011327189  eval_loss:0.37601623\n",
      "epoch:90  train_loss:0.011613186  eval_loss:0.38511124\n",
      "epoch:92  train_loss:0.011995236  eval_loss:0.3930485\n",
      "epoch:94  train_loss:0.012521706  eval_loss:0.4003705\n",
      "epoch:96  train_loss:0.0132319145  eval_loss:0.40774095\n",
      "epoch:98  train_loss:0.014016878  eval_loss:0.41302964\n",
      "CPU times: user 34.1 s, sys: 1.78 s, total: 35.9 s\n",
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%time train(mlp, optimizer, criterion, train_dataloader, eval_dataloader, epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train(mlp, optimizer, criterion, train_dataloader, eval_dataloader, epochs=max_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "trainable_params = mlp.trainable_params()\n",
    "print(mlp)\n",
    "trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindnlp\n",
    "\n",
    "\n",
    "import mindnlp.peft as peft\n",
    "\n",
    "# target_modules are modules to add PEFT params\n",
    "# modules_to_save are original modules, not freezed.\n",
    "config = peft.LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"layers.0\", \"layers.2\"],\n",
    "    modules_to_save=[\"layers.4\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "peft_mlp = mindnlp.peft.get_peft_model(mlp, peft_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_mlp.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn, ops\n",
    "\n",
    "class TEMP():\n",
    "    def __init__(self,):\n",
    "        self.r = {}\n",
    "\n",
    "class TestModel(nn.Dense, TEMP):\n",
    "    def __init__(self, ):\n",
    "        nn.Dense.__init__(self, 10, 20, has_bias=False)\n",
    "        TEMP.__init__(self,)\n",
    "\n",
    "    \n",
    "    def construct(self, x: mindspore.Tensor):\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "x = ops.rand((1000,10), seed=0)\n",
    "model = TestModel()\n",
    "model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
