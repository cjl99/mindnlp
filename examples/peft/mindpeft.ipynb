{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjl/code/mind/mindnlp/mindnlp/utils/download.py:29: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "from mindspore import context, Tensor\n",
    "from mindspore.communication import init, get_rank\n",
    "\n",
    "from mindnlp.peft import LoraConfig, LoraModel\n",
    "\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")\n",
    "# init('nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32\n",
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<mindspore.dataset.engine.datasets_user_defined.GeneratorDataset at 0x7f323c15ddc0>,\n",
       " <mindspore.dataset.engine.datasets.BatchDataset at 0x7f31bffae310>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mindspore 构造一个测试数据集\n",
    "\n",
    "X = mindspore.ops.rand((1000,20), seed=0)\n",
    "y = (X.sum(1) > 10).int()\n",
    "\n",
    "print(ops.DType()(X))\n",
    "print(y.shape)\n",
    "\n",
    "n_train = 800\n",
    "batch_size = 64\n",
    "\n",
    "class MyIterable:\n",
    "    def __init__(self, X: Tensor, y:Tensor):\n",
    "        self._index = 0\n",
    "        self._data = X\n",
    "        self._label = y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._index >= len(self._data):\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            item = (self._data[self._index], self._label[self._index])\n",
    "            self._index += 1\n",
    "            return item\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._index = 0\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "\n",
    "train_ds = ds.GeneratorDataset(\n",
    "    source=MyIterable(X[:n_train], y[:n_train]),\n",
    "    column_names=[\"data\", \"label\"],\n",
    "    shuffle=True\n",
    ")\n",
    "eval_ds = ds.GeneratorDataset(\n",
    "    source=MyIterable(X[n_train:], y[n_train:]),\n",
    "    column_names=[\"data\", \"label\"],\n",
    ")\n",
    "\n",
    "train_dataloader = train_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "eval_dataloader = eval_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "train_ds, train_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_net_params(net: nn.Cell):\n",
    "    all_parameter = []\n",
    "    for item in net.get_parameters():\n",
    "        all_parameter.append(item)\n",
    "        print(item.name, item.data.shape)\n",
    "    print(f\"all parameter numbers: {len(all_parameter)}\")\n",
    "\n",
    "    # Obtain trainable parameters.\n",
    "    trainable_params = net.trainable_params()\n",
    "    for item in trainable_params:\n",
    "        print(item.name, item.data.shape)\n",
    "    print(f\"trainable parameter numbers: {len(trainable_params)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Cell):\n",
    "    def __init__(self, hidden=2000):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.SequentialCell(\n",
    "            nn.Dense(20, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(hidden, 2),\n",
    "            nn.LogSoftmax(axis=-1)\n",
    "        )\n",
    "\n",
    "    def construct(self, X):\n",
    "        return self.layers(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight (2000, 20)\n",
      "layers.0.bias (2000,)\n",
      "layers.2.weight (2000, 2000)\n",
      "layers.2.bias (2000,)\n",
      "layers.4.weight (2, 2000)\n",
      "layers.4.bias (2,)\n",
      "all parameter numbers: 6\n",
      "layers.0.weight (2000, 20)\n",
      "layers.0.bias (2000,)\n",
      "layers.2.weight (2000, 2000)\n",
      "layers.2.bias (2000,)\n",
      "layers.4.weight (2, 2000)\n",
      "layers.4.bias (2,)\n",
      "trainable parameter numbers: 6\n"
     ]
    }
   ],
   "source": [
    "lr = 0.002\n",
    "batch_size = 64\n",
    "max_epochs = 50\n",
    "\n",
    "mlp = MLP()\n",
    "print_net_params(mlp)\n",
    "optimizer = nn.Adam(mlp.trainable_params(), learning_rate=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, optimizer, criterion, train_dataloader, eval_dataloader, epochs):\n",
    "    def forward_fn(data, label):\n",
    "        logits = model(data)\n",
    "        loss = criterion(logits, label)\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "    def train_one_epoch():\n",
    "        model.set_train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in train_dataloader:\n",
    "            # forward + compute grad\n",
    "            (loss, logits), grad = grad_fn(xb, yb)\n",
    "            # update model params\n",
    "            optimizer(grad)\n",
    "            total_loss += loss\n",
    "        \n",
    "        return total_loss / len(train_dataloader)\n",
    "    \n",
    "    def eval_one_epoch():\n",
    "        model.set_train(False)\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in eval_dataloader:\n",
    "            (loss, logits), grad = grad_fn(xb, yb)\n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss / len(eval_dataloader)\n",
    "\n",
    "    # train start from here\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch()\n",
    "        eval_loss = eval_one_epoch()\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"epoch:{epoch}  train_loss:{train_loss}  eval_loss:{eval_loss}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0  train_loss:0.69207174  eval_loss:0.6788408\n",
      "epoch:2  train_loss:0.6668038  eval_loss:0.7038066\n",
      "epoch:4  train_loss:0.42241004  eval_loss:0.37191167\n",
      "epoch:6  train_loss:0.2422852  eval_loss:0.24100828\n",
      "epoch:8  train_loss:0.13968705  eval_loss:0.30576023\n",
      "epoch:10  train_loss:0.19051766  eval_loss:0.3676723\n",
      "epoch:12  train_loss:0.15267205  eval_loss:0.20878808\n",
      "epoch:14  train_loss:0.23691426  eval_loss:0.3014338\n",
      "epoch:16  train_loss:0.5297999  eval_loss:0.6039374\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "%time train(mlp, optimizer, criterion, train_dataloader, eval_dataloader, epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP<\n",
      "  (layers): SequentialCell<\n",
      "    (0): Dense<input_channels=20, output_channels=2000, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=2000, output_channels=2000, has_bias=True>\n",
      "    (3): ReLU<>\n",
      "    (4): Dense<input_channels=2000, output_channels=2, has_bias=True>\n",
      "    (5): LogSoftmax<>\n",
      "    >\n",
      "  >\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter (name=layers.0.weight, shape=(2000, 20), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=layers.0.bias, shape=(2000,), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=layers.2.weight, shape=(2000, 2000), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=layers.2.bias, shape=(2000,), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=layers.4.weight, shape=(2, 2000), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=layers.4.bias, shape=(2,), dtype=Float32, requires_grad=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "trainable_params = mlp.trainable_params()\n",
    "print(mlp)\n",
    "trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindnlp\n",
    "import mindnlp.peft as peft\n",
    "\n",
    "\n",
    "# target_modules are modules to add PEFT params\n",
    "# modules_to_save are original modules, not freezed.\n",
    "config = peft.LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"layers.0\", \"layers.2\"],\n",
    "    modules_to_save=[\"layers.4\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(base_model_name_or_path=None, peft_type=<PeftType.LORA: 'LORA'>, task_type=None, inference_mode=False, r=8, target_modules=['layers.0', 'layers.2'], lora_alpha=None, lora_dropout=None, fan_in_fan_out=False, bias='none', modules_to_save=['layers.4'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LoraConfig' object has no attribute 'is_prompt_learning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mlp \u001b[39m=\u001b[39m MLP()\n\u001b[0;32m----> 2\u001b[0m peft_mlp \u001b[39m=\u001b[39m peft\u001b[39m.\u001b[39;49mget_peft_model(mlp, peft_config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/mapping.py:88\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m     85\u001b[0m peft_config\u001b[39m.\u001b[39mbase_model_name_or_path \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     87\u001b[0m \u001b[39m# no specific task_type and is not prompt_learning\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m \u001b[39mif\u001b[39;00m peft_config\u001b[39m.\u001b[39mtask_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[39m.\u001b[39mkeys() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m peft_config\u001b[39m.\u001b[39;49mis_prompt_learning:\n\u001b[1;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m PeftModel(model, peft_config, adapter_name\u001b[39m=\u001b[39madapter_name) \n\u001b[1;32m     91\u001b[0m \u001b[39m# TODO: prompt learning\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m# if peft_config.is_prompt_learning:\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m#     # peft_config = _prepare_prompt_learning_config(peft_config, model_config)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LoraConfig' object has no attribute 'is_prompt_learning'"
     ]
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "peft_mlp = peft.get_peft_model(mlp, peft_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
