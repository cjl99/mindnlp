{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "from mindspore import context, Tensor\n",
    "from mindspore.communication import init, get_rank\n",
    "\n",
    "\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")\n",
    "# init('nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32\n",
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<mindspore.dataset.engine.datasets_user_defined.GeneratorDataset at 0x7ff9190a0f70>,\n",
       " <mindspore.dataset.engine.datasets.BatchDataset at 0x7ff9190a0fd0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mindspore 构造一个测试数据集\n",
    "\n",
    "X = mindspore.ops.rand((1000,20), seed=0)\n",
    "y = (X.sum(1) > 10).int()\n",
    "\n",
    "print(ops.DType()(X))\n",
    "print(y.shape)\n",
    "\n",
    "n_train = 800\n",
    "batch_size = 64\n",
    "\n",
    "class MyIterable:\n",
    "    def __init__(self, X: Tensor, y:Tensor):\n",
    "        self._index = 0\n",
    "        self._data = X\n",
    "        self._label = y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._index >= len(self._data):\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            item = (self._data[self._index], self._label[self._index])\n",
    "            self._index += 1\n",
    "            return item\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._index = 0\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "\n",
    "train_ds = ds.GeneratorDataset(\n",
    "    source=MyIterable(X[:n_train], y[:n_train]),\n",
    "    column_names=[\"data\", \"label\"],\n",
    "    shuffle=True\n",
    ")\n",
    "eval_ds = ds.GeneratorDataset(\n",
    "    source=MyIterable(X[n_train:], y[n_train:]),\n",
    "    column_names=[\"data\", \"label\"],\n",
    ")\n",
    "\n",
    "train_dataloader = train_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "eval_dataloader = eval_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "train_ds, train_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_net_params(net: nn.Cell):\n",
    "    all_parameter = []\n",
    "    for item in net.get_parameters():\n",
    "        all_parameter.append(item)\n",
    "        print(item.name, item.data.shape)\n",
    "    print(f\"all parameter numbers: {len(all_parameter)}\")\n",
    "\n",
    "    # Obtain trainable parameters.\n",
    "    trainable_params = net.trainable_params()\n",
    "    for item in trainable_params:\n",
    "        print(item.name, item.data.shape)\n",
    "    print(f\"trainable parameter numbers: {len(trainable_params)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Cell):\n",
    "    def __init__(self, hidden=2000):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.SequentialCell(\n",
    "            nn.Dense(20, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(hidden, 2),\n",
    "            nn.LogSoftmax(axis=-1)\n",
    "        )\n",
    "\n",
    "    def construct(self, X):\n",
    "        return self.layers(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight (2000, 20)\n",
      "layers.0.bias (2000,)\n",
      "layers.2.weight (2000, 2000)\n",
      "layers.2.bias (2000,)\n",
      "layers.4.weight (2, 2000)\n",
      "layers.4.bias (2,)\n",
      "all parameter numbers: 6\n",
      "layers.0.weight (2000, 20)\n",
      "layers.0.bias (2000,)\n",
      "layers.2.weight (2000, 2000)\n",
      "layers.2.bias (2000,)\n",
      "layers.4.weight (2, 2000)\n",
      "layers.4.bias (2,)\n",
      "trainable parameter numbers: 6\n"
     ]
    }
   ],
   "source": [
    "lr = 0.002\n",
    "batch_size = 64\n",
    "max_epochs = 50\n",
    "\n",
    "mlp = MLP()\n",
    "print_net_params(mlp)\n",
    "optimizer = nn.Adam(mlp.trainable_params(), learning_rate=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, optimizer, criterion, train_dataloader, eval_dataloader, epochs):\n",
    "    def forward_fn(data, label):\n",
    "        logits = model(data)\n",
    "        loss = criterion(logits, label)\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "    def train_one_epoch():\n",
    "        model.set_train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in train_dataloader:\n",
    "            # forward + compute grad\n",
    "            (loss, logits), grad = grad_fn(xb, yb)\n",
    "            # update model params\n",
    "            optimizer(grad)\n",
    "            total_loss += loss\n",
    "        \n",
    "        return total_loss / len(train_dataloader)\n",
    "    \n",
    "    def eval_one_epoch():\n",
    "        model.set_train(False)\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in eval_dataloader:\n",
    "            (loss, logits), grad = grad_fn(xb, yb)\n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss / len(eval_dataloader)\n",
    "\n",
    "    # train start from here\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch()\n",
    "        eval_loss = eval_one_epoch()\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"epoch:{epoch}  train_loss:{train_loss}  eval_loss:{eval_loss}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time train(mlp, optimizer, criterion, train_dataloader, eval_dataloader, epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP<\n",
      "  (layers): SequentialCell<\n",
      "    (0): Dense<input_channels=20, output_channels=2000, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=2000, output_channels=2000, has_bias=True>\n",
      "    (3): ReLU<>\n",
      "    (4): Dense<input_channels=2000, output_channels=2, has_bias=True>\n",
      "    (5): LogSoftmax<>\n",
      "    >\n",
      "  >\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter (name=layers.0.weight, shape=(2000, 20), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=layers.0.bias, shape=(2000,), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=layers.2.weight, shape=(2000, 2000), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=layers.2.bias, shape=(2000,), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=layers.4.weight, shape=(2, 2000), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=layers.4.bias, shape=(2,), dtype=Float32, requires_grad=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "trainable_params = mlp.trainable_params()\n",
    "print(mlp)\n",
    "trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjl/code/mind/mindnlp/mindnlp/utils/download.py:29: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import mindnlp\n",
    "\n",
    "\n",
    "import mindnlp.peft as peft\n",
    "\n",
    "# target_modules are modules to add PEFT params\n",
    "# modules_to_save are original modules, not freezed.\n",
    "config = peft.LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"layers.0\", \"layers.2\"],\n",
    "    modules_to_save=[\"layers.4\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, base_model_name_or_path=None, task_type=None, inference_mode=False, r=8, target_modules=['layers.0', 'layers.2'], lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', modules_to_save=['layers.4'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace  0  new module  Linear<input_channels=20, output_channels=2000, has_bias=True>\n",
      "replace  2  new module  Linear<input_channels=2000, output_channels=2000, has_bias=True>\n",
      "====debug==== OKKKKKKK before add modules to save\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "peft_mlp = mindnlp.peft.get_peft_model(mlp, peft_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The 'MLP' object has no attribute 'named_parameters'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/peft_model.py:180\u001b[0m, in \u001b[0;36mPeftModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getattr__\u001b[39;49m(name)  \u001b[39m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mindnlp/lib/python3.9/site-packages/mindspore/nn/cell.py:336\u001b[0m, in \u001b[0;36mCell.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[39mreturn\u001b[39;00m ParameterTuple(params_list[name])\n\u001b[0;32m--> 336\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: The 'PeftModel' object has no attribute 'named_parameters'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/tuners/lora.py:292\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getattr__\u001b[39;49m(name)  \u001b[39m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mindnlp/lib/python3.9/site-packages/mindspore/nn/cell.py:336\u001b[0m, in \u001b[0;36mCell.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[39mreturn\u001b[39;00m ParameterTuple(params_list[name])\n\u001b[0;32m--> 336\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: The 'LoraModel' object has no attribute 'named_parameters'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m peft_mlp\u001b[39m.\u001b[39;49mprint_trainable_parameters()\n",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/peft_model.py:164\u001b[0m, in \u001b[0;36mPeftModel.print_trainable_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m trainable_params \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    163\u001b[0m all_param \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 164\u001b[0m \u001b[39mfor\u001b[39;00m _, param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnamed_parameters():\n\u001b[1;32m    165\u001b[0m     num_params \u001b[39m=\u001b[39m param\u001b[39m.\u001b[39mnumel()\n\u001b[1;32m    166\u001b[0m     \u001b[39m# if using DS Zero 3 and the weights are initialized empty\u001b[39;00m\n",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/peft_model.py:182\u001b[0m, in \u001b[0;36mPeftModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattr__\u001b[39m(name)  \u001b[39m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model, name)\n",
      "File \u001b[0;32m~/code/mind/mindnlp/mindnlp/peft/tuners/lora.py:294\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattr__\u001b[39m(name)  \u001b[39m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, name)\n",
      "File \u001b[0;32m~/anaconda3/envs/mindnlp/lib/python3.9/site-packages/mindspore/nn/cell.py:336\u001b[0m, in \u001b[0;36mCell.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m params_list:\n\u001b[1;32m    335\u001b[0m         \u001b[39mreturn\u001b[39;00m ParameterTuple(params_list[name])\n\u001b[0;32m--> 336\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: The 'MLP' object has no attribute 'named_parameters'."
     ]
    }
   ],
   "source": [
    "peft_mlp.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
